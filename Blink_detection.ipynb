{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting blinking using a videostream with dlib and OpenCV"
      ],
      "metadata": {
        "id": "8ilSBfegZ8zV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After making the code for detecting open and closed eyes through images, I would like to use a video and detect the time each blink.\n",
        "\n",
        "The code detect the blinks, but also calulates the time between each blink. If the person has not blinked for more than 10 seconds, the \"please blink\" message gets printed."
      ],
      "metadata": {
        "id": "f0JdDckGZ-sz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QmkH1f9Zmjh",
        "outputId": "e2f39f3f-276f-4d0c-dc2e-1a7fa858c251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 08:12:54--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n",
            "--2025-02-10 08:12:54--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  17.9MB/s    in 4.4s    \n",
            "\n",
            "2025-02-10 08:13:00 (13.9 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "import time\n",
        "\n",
        "#Download the shape predictor file\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "#Load the facial landmark predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download from dlib repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Eye aspect ratio (EAR) calculation\n",
        "def eye_aspect_ratio(eye_points):\n",
        "    A = distance.euclidean(eye_points[1], eye_points[5])\n",
        "    B = distance.euclidean(eye_points[2], eye_points[4])\n",
        "    C = distance.euclidean(eye_points[0], eye_points[3])\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "#Thresholds\n",
        "EAR_THRESHOLD = 0.25  #Below this, eyes are considered closed\n",
        "BLINK_REMINDER_THRESHOLD = 10  #Alert if no blink for 10 seconds\n",
        "\n",
        "#Video capture\n",
        "cap = cv2.VideoCapture('/content/20250208_111726.mp4')\n",
        "output_video_path = \"/content/output_with_blinks.mp4\"\n",
        "\n",
        "#Get video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "print(f\"FPS: {fps}\")\n",
        "\n",
        "#Define codec and initialize VideoWriter\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"avc1\") #or mp4v or avc1\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "#Blink tracking variables\n",
        "blink_count = 0\n",
        "blink_start_time = None\n",
        "blink_detected = False\n",
        "blink_alert_shown = False\n",
        "show_warning = False\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    #Rotate the frame for proper orientation (If the video is vertical)\n",
        "    #frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    #Get the current frame timestamp in seconds\n",
        "    frame_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
        "\n",
        "    #If working with webcam then replace frame time with current time\n",
        "    #current_time = time.time()\n",
        "\n",
        "    #Convert frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "\n",
        "        #Extract eye landmarks\n",
        "        left_eye_pts = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)]\n",
        "        right_eye_pts = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)]\n",
        "\n",
        "        #Compute EAR for both eyes\n",
        "        left_ear = eye_aspect_ratio(left_eye_pts)\n",
        "        right_ear = eye_aspect_ratio(right_eye_pts)\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        #Detect blink\n",
        "        if avg_ear < EAR_THRESHOLD:\n",
        "            if not blink_detected:  #New blink detected\n",
        "                blink_count += 1\n",
        "                blink_detected = True\n",
        "                show_warning = False  #reset warning when blink detected\n",
        "\n",
        "                #Calculate time since last blink\n",
        "                if blink_start_time is not None:\n",
        "                    time_between_blinks = frame_time - blink_start_time\n",
        "                    print(f\"Blink #{blink_count}: {time_between_blinks:.2f} seconds since last blink\")\n",
        "                else:\n",
        "                    print(f\"Blink #{blink_count}: First blink detected!\")\n",
        "\n",
        "                blink_start_time = frame_time\n",
        "                blink_alert_shown = False  #Reset alert flag\n",
        "        else:\n",
        "            blink_detected = False  #Reset when eyes reopen\n",
        "\n",
        "    #Check if more than 10 seconds have passed since last blink\n",
        "    if blink_start_time and (frame_time - blink_start_time) >= BLINK_REMINDER_THRESHOLD:\n",
        "        if not blink_alert_shown:  #Show only once per reminder\n",
        "            print(\"⚠️ BLINK PLEASE! You haven't blinked in the past 10+ seconds! ⚠️\")\n",
        "            blink_alert_shown = True\n",
        "            show_warning = True  #Activate warning display\n",
        "\n",
        "    #Display blink count on video\n",
        "    cv2.putText(frame, f\"Blinks: {blink_count}\", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    #Show warning message only when needed\n",
        "    if show_warning:\n",
        "        cv2.putText(frame, \" BLINK! \", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 4)\n",
        "\n",
        "    #Rotate back before saving (for proper playback, if video is vertical)\n",
        "    #frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "    #Write the frame to output video\n",
        "    out.write(frame)\n",
        "\n",
        "    #Show the video feed for debugging\n",
        "    #cv2_imshow(frame)\n",
        "\n",
        "\n",
        "#cap.release()\n",
        "#out.release()\n",
        "\n",
        "print(f\"Video saved as {output_video_path}\")"
      ],
      "metadata": {
        "id": "NBB1SUyNaGLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting open or closed eyes from images using dlib"
      ],
      "metadata": {
        "id": "9MIe790idUbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eye_aspect_ratio(eye_points):\n",
        "    \"\"\"Calculate the Eye Aspect Ratio (EAR) to detect if an eye is open or closed\"\"\"\n",
        "    A = distance.euclidean(eye_points[1], eye_points[5])\n",
        "    B = distance.euclidean(eye_points[2], eye_points[4])\n",
        "    C = distance.euclidean(eye_points[0], eye_points[3])\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "# Threshold to classify eye state\n",
        "EAR_THRESHOLD = 0.25\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread(\"/content/20250128_113657.jpg\")\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces\n",
        "faces = detector(gray)\n",
        "\n",
        "for face in faces:\n",
        "    landmarks = predictor(gray, face)\n",
        "\n",
        "    # Extract eye landmarks (dlib indices)\n",
        "    left_eye_pts = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)]\n",
        "    right_eye_pts = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)]\n",
        "\n",
        "    # Compute EAR for both eyes\n",
        "    left_ear = eye_aspect_ratio(left_eye_pts)\n",
        "    right_ear = eye_aspect_ratio(right_eye_pts)\n",
        "\n",
        "    # Average EAR\n",
        "    avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "    # Determine eye state\n",
        "    if avg_ear < EAR_THRESHOLD:\n",
        "        state = \"Closed\"\n",
        "    else:\n",
        "        state = \"Open\"\n",
        "\n",
        "    print(f\"Eye state: {state}\")\n",
        "\n",
        "# Show the image\n",
        "#cv2_imshow(image)"
      ],
      "metadata": {
        "id": "vInzhVKydUrQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}